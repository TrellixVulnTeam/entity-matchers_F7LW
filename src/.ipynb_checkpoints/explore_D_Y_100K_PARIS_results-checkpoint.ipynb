{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration of results for PARIS, using code provided by authors (Zequn Sun) and Manuel and Stefano.\n",
    "\n",
    "### 12th October 2020\n",
    "After having downloaded the code provided by the authors and changed the directories for the datasets with more appropriate command line arguments, we have run experiments on PARIS, using as test data the `test_links` contained in folder 1 in the dataset D_Y_100K_V1 using the pipeline provided by the author:\n",
    "\n",
    "- 1) You should first convert our dataset into \".nt\" files using the python code \"nt_file.py\". The folder \"nt_datasets/\" contains the \".nt\" files of our D_Y datasets. You can directly use them.\n",
    "\n",
    "- 2) Run PARIS on the generated \".nt\" files using \"paris_interface.py\". The code will output a folder like \"D_Y_100K_V1_1010_224118_wjrevhtsrq\" which contains the raw results of PARIS.\n",
    "\n",
    "- 3) Use \"paris_results.py\" to get the evaluation results based on the output of PARIS and the gold standard (e.g., all alignment or test alignment).\n",
    "\n",
    "This lead us to the results (absolutely in line with what the authors claim) of: \n",
    "\n",
    "```\n",
    "P: 60545/70193=0.8626\n",
    "R: 60545/70000=0.8649\n",
    "F1: 0.8637\n",
    "```\n",
    "\n",
    "We still had the problem that we have run the experiments with our own wrapper, and results did not match at all. In order to be sure of our claim, we run our own pipeline (executable with `python /src/main.py --command-line-arguments...`) and observed totally different results, namely:\n",
    "\n",
    "```\n",
    "P: 59983/61095=0.9818\n",
    "R: 59983/70000=0.8569\n",
    "F1: 0.9151\n",
    "```\n",
    "\n",
    "Although the recall is quite similar, the precision achieved surprisingly better results (and consequently the f1-score). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis \n",
    "\n",
    "The difference is huge, if we think about the fact that although the two pipelines were different, they run on the same data and execute the same algorithm (`paris.jar` as provided by the authors of PARIS). \n",
    "\n",
    "Analysing the source code of the pipeline provided kindly us by Zequn Sun, we spotted the following differences with ours:\n",
    "\n",
    "1) A prefix for the dataset yago of `y2`. This should not be a problem in practice, since even in our simulation we added the prefix dbp: for dbpedia, but it may improve results for PARIS which makes some kind of checks (we can see it in the source code of PARIS, although it is not easy to tell what kind of improvements it can cause -contained in `standardPrefixes`).\n",
    "\n",
    "2) Precision computed over `eqv_full_i.tsv`, whereas ours was computed over `eqv_i.tsv`.\n",
    "\n",
    "The second point, actually, deserves a more in depth discussion: when PARIS executes, it prints its alignments in two files, `eqv_i.tsv` and `eqv_full_i.tsv`. Those two files have different meanings and purposes: the second contains all the alignments that PAIRS considers interesting, even if it means that the same entity appears more than once. A good example is the entity `Baltimora` which correctly aligned in `D_Y_15K_V1` with Baltimora, but even with Washington, which is a city that lies approximately in the same geographical area. `eqv_i.tsv`, instead, contains the most likely alingments for all entities (so, duplicates are dropped in favour of the aligmnent which has the highest probability).\n",
    "\n",
    "For this reason, we believed it is more convenient (and correct), to perform the alignment on the `eqv_i.tsv` file, and not on the `full` version.\n",
    "\n",
    "Still, curious to understand the decision that brought the authors to pick the larger version, we checked their source code for `paris_results.py` and found out that in fact they remove duplicates picking the ones with highest probability. This lead us to believe that the difference in performance needed even more in-depth analysis: first, we run our pipeline with the dataset created by the author's pipeline, and managed to replicate precisely the previous results close to 98% inprecision (which were the results computed by our pipeline in the first place). Secondly, we run the author's pipeline, providing the dataset created by our pipeline (hence without the `y2` prefix for Yago entities), and managed to replicate precisely their results (89% in precision). - This, in turn, leads us to believe that the `y2` prefix is not an important addition, and can be taken out of consideration for the time being.\n",
    "\n",
    "At the same time, by using the results computed entirely with the author's pipeline, and switching to the `eqv_i.tsv` file during the evaluation phase, raised again the precision to 98%.\n",
    "\n",
    "We try to understand why this happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We load the results for PARIS that perform both 98% and 86% on the author's pipeline.\n",
    "eqv_i_full_df = pd.read_csv(\"../paris_OpenEA/D_Y_100K_V1_1013_005128_rrijkskrfg/output/9_eqv_full.tsv\", \n",
    "                            sep='\\t', \n",
    "                            names=['DB1', 'DB2', 'Probability'])\n",
    "eqv_i_df = pd.read_csv(\"../paris_OpenEA/D_Y_100K_V1_1013_005128_rrijkskrfg/output/9_eqv.tsv\", \n",
    "                       sep='\\t',  \n",
    "                       names=['DB1', 'DB2', 'Probability'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of eqv_full is 97056\n"
     ]
    }
   ],
   "source": [
    "print(\"The size of eqv_full is {}\".format(len(eqv_i_full_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of eqv is 86962\n"
     ]
    }
   ],
   "source": [
    "print(\"The size of eqv is {}\".format(len(eqv_i_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the difference is quite large, about 10k entitites.\n",
    "Among these, we want to retain only the entities that are contained in the `test_links` file, which is the one for fold 1 of `D_Y_100K_V1/5_721/1/test_links` (since the other tuples are simply not considered at all, so they would be considered injoustly as wrong tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan in DB1: 1\n",
      "nan in DB2: 0\n"
     ]
    }
   ],
   "source": [
    "# Count how many nans there are:\n",
    "print(\"nan in DB1: {}\\nnan in DB2: {}\".format(np.sum(eqv_i_df['DB1'].isna()), np.sum(eqv_i_df['DB2'].isna())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan in DB1: 1\n",
      "nan in DB2: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"nan in DB1: {}\\nnan in DB2: {}\".format(np.sum(eqv_i_full_df['DB1'].isna()), np.sum(eqv_i_full_df['DB2'].isna())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is only one nan, we can safely remove it without affecting performance too much!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "eqv_i_df = eqv_i_df.dropna()\n",
    "eqv_i_full_df = eqv_i_full_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DB1</th>\n",
       "      <th>DB2</th>\n",
       "      <th>Probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dbp:resource/Menahem_Golan</td>\n",
       "      <td>y2:Menahem_Golan</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dbp:resource/Snow_White_(1987_film)</td>\n",
       "      <td>y2:Snow_White_(1987_film)</td>\n",
       "      <td>0.509527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dbp:resource/Germi_County</td>\n",
       "      <td>y2:Germi_County</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dbp:resource/Chat_Qeshlaq-e_Bala</td>\n",
       "      <td>y2:Chat_Qeshlaq-e_Bala</td>\n",
       "      <td>0.807499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dbp:resource/All_Hell's_Breakin'_Loose</td>\n",
       "      <td>y2:All_Hell's_Breakin'_Loose</td>\n",
       "      <td>0.677578</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      DB1                           DB2  \\\n",
       "0              dbp:resource/Menahem_Golan              y2:Menahem_Golan   \n",
       "1     dbp:resource/Snow_White_(1987_film)     y2:Snow_White_(1987_film)   \n",
       "2               dbp:resource/Germi_County               y2:Germi_County   \n",
       "3        dbp:resource/Chat_Qeshlaq-e_Bala        y2:Chat_Qeshlaq-e_Bala   \n",
       "4  dbp:resource/All_Hell's_Breakin'_Loose  y2:All_Hell's_Breakin'_Loose   \n",
       "\n",
       "   Probability  \n",
       "0     1.000000  \n",
       "1     0.509527  \n",
       "2     1.000000  \n",
       "3     0.807499  \n",
       "4     0.677578  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eqv_i_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, we need to substitute `dbp:resource` with `http://dbpedia.org/resource` and `y2` with \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DB1</th>\n",
       "      <th>DB2</th>\n",
       "      <th>Probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://dbpedia.org/resource/Menahem_Golan</td>\n",
       "      <td>Menahem_Golan</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://dbpedia.org/resource/Snow_White_(1987_f...</td>\n",
       "      <td>Snow_White_(1987_film)</td>\n",
       "      <td>0.509527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://dbpedia.org/resource/Germi_County</td>\n",
       "      <td>Germi_County</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://dbpedia.org/resource/Chat_Qeshlaq-e_Bala</td>\n",
       "      <td>Chat_Qeshlaq-e_Bala</td>\n",
       "      <td>0.807499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://dbpedia.org/resource/All_Hell's_Breakin...</td>\n",
       "      <td>All_Hell's_Breakin'_Loose</td>\n",
       "      <td>0.677578</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 DB1  \\\n",
       "0          http://dbpedia.org/resource/Menahem_Golan   \n",
       "1  http://dbpedia.org/resource/Snow_White_(1987_f...   \n",
       "2           http://dbpedia.org/resource/Germi_County   \n",
       "3    http://dbpedia.org/resource/Chat_Qeshlaq-e_Bala   \n",
       "4  http://dbpedia.org/resource/All_Hell's_Breakin...   \n",
       "\n",
       "                         DB2  Probability  \n",
       "0              Menahem_Golan     1.000000  \n",
       "1     Snow_White_(1987_film)     0.509527  \n",
       "2               Germi_County     1.000000  \n",
       "3        Chat_Qeshlaq-e_Bala     0.807499  \n",
       "4  All_Hell's_Breakin'_Loose     0.677578  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eqv_i_df['DB1'] = eqv_i_df['DB1'].apply(lambda x: x.replace(\"dbp:resource\", \"http://dbpedia.org/resource\"))\n",
    "eqv_i_df['DB2'] = eqv_i_df['DB2'].apply(lambda x: x.replace(\"y2:\", \"\"))\n",
    "eqv_i_full_df['DB1'] = eqv_i_full_df['DB1'].apply(lambda x: x.replace(\"dbp:resource\", \"http://dbpedia.org/resource\"))\n",
    "eqv_i_full_df['DB2'] = eqv_i_full_df['DB2'].apply(lambda x: x.replace(\"y2:\", \"\"))\n",
    "eqv_i_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_links_df = pd.read_csv(\"../datasets/OpenEA_dataset/D_Y_100K_V1/721_5fold/1/test_links\",\n",
    "                            sep='\\t',\n",
    "                            names=['DB1', 'DB2'])\n",
    "assert(len(test_links_df)==70000)      # Check that the length is indeed 70k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DB1</th>\n",
       "      <th>DB2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://dbpedia.org/resource/Suffragette_(film)</td>\n",
       "      <td>Suffragette_(film)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://dbpedia.org/resource/Telegram_Sam</td>\n",
       "      <td>Telegram_Sam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://dbpedia.org/resource/Barnaul</td>\n",
       "      <td>Barnaul</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://dbpedia.org/resource/Pontifical_Atheneu...</td>\n",
       "      <td>Pontifical_Atheneum_of_St._Anselm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://dbpedia.org/resource/Pablo_Bezombe</td>\n",
       "      <td>Pablo_Bezombe</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 DB1  \\\n",
       "0     http://dbpedia.org/resource/Suffragette_(film)   \n",
       "1           http://dbpedia.org/resource/Telegram_Sam   \n",
       "2                http://dbpedia.org/resource/Barnaul   \n",
       "3  http://dbpedia.org/resource/Pontifical_Atheneu...   \n",
       "4          http://dbpedia.org/resource/Pablo_Bezombe   \n",
       "\n",
       "                                 DB2  \n",
       "0                 Suffragette_(film)  \n",
       "1                       Telegram_Sam  \n",
       "2                            Barnaul  \n",
       "3  Pontifical_Atheneum_of_St._Anselm  \n",
       "4                      Pablo_Bezombe  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_links_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print the alignments that we would end up with if we drop rows that are not in the ground truth in three cases:\n",
    "\n",
    "1) only the first column is in the ground truth\n",
    "\n",
    "2) Either the first either the second column is in the ground truth\n",
    "\n",
    "3) Both of the columns are in the ground truth\n",
    "\n",
    "The number of alignments considered by the author's approach is 60734."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# alignments where only DB1 appears in gold_standard: 60734\n",
      "# alignments where either DB1 either DB2 appear in gold_standard: 60980\n",
      "# alignments where both DB1 and DB2 appear in gold_standard: 60503\n"
     ]
    }
   ],
   "source": [
    "print(\"# alignments where only DB1 appears in gold_standard: {}\".format(\n",
    "    len(eqv_i_df.loc[eqv_i_df['DB1'].isin(test_links_df['DB1'])])))\n",
    "print(\"# alignments where either DB1 either DB2 appear in gold_standard: {}\".format(\n",
    "    len(eqv_i_df.loc[eqv_i_df['DB1'].isin(test_links_df['DB1']) | eqv_i_df['DB2'].isin(test_links_df['DB2'])])))\n",
    "print(\"# alignments where both DB1 and DB2 appear in gold_standard: {}\".format(\n",
    "    len(eqv_i_df.loc[eqv_i_df['DB1'].isin(test_links_df['DB1']) & eqv_i_df['DB2'].isin(test_links_df['DB2'])])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# alignments where only DB1 appears in gold_standard: 67834\n",
      "# alignments where either DB1 either DB2 appear in gold_standard: 70002\n",
      "# alignments where both DB1 and DB2 appear in gold_standard: 65602\n"
     ]
    }
   ],
   "source": [
    "print(\"# alignments where only DB1 appears in gold_standard: {}\".format(\n",
    "    len(eqv_i_full_df.loc[eqv_i_full_df['DB1'].isin(test_links_df['DB1'])])))\n",
    "print(\"# alignments where either DB1 either DB2 appear in gold_standard: {}\".format(\n",
    "    len(eqv_i_full_df.loc[eqv_i_full_df['DB1'].isin(test_links_df['DB1']) | eqv_i_full_df['DB2'].isin(test_links_df['DB2'])])))\n",
    "print(\"# alignments where both DB1 and DB2 appear in gold_standard: {}\".format(\n",
    "    len(eqv_i_full_df.loc[eqv_i_full_df['DB1'].isin(test_links_df['DB1']) & eqv_i_full_df['DB2'].isin(test_links_df['DB2'])])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence we may conclude that they keep only alignments for which the first column is contained in the ground truth. This does not seem correct: the two columns are pretty much symmetric, so there is no reason why pivoting results based only on the first column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional note which may be a useful observation: number of alignments oputputted by PARIS varies from execution to execution: this should not be surprising, since the execution is randomized and hence may vary a little by execution to execution. This could be easily fixed by setting a constant seed for the PARIS algorithm to run with, but for now it should not make any difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting the duplicates in `_full` for the first column and the second.\n",
    "\n",
    "Maybe the asymmetry while computing the set of alignments for the author's approach is the reason of why it performs so poorly on `_full`. \n",
    "In order to analyze it deeper, we perform the same set of actions that they do for the `_full` dataset, and see if the result contains duplicates in the second column (an event which is totally avoided using the not-full dataset, which already performs some kind of selection (which one?))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is almost copy pasted (added prints) from the source code provided by the authors.\n",
    "def read_paris_mappings(file_path, standard_links):\n",
    "    print(\"len of standard_links {}\".format(len(standard_links)))\n",
    "    pair_sim_set = set()\n",
    "    ent_set = set(e1 for (e1, _) in standard_links) | set(e2 for (_, e2) in standard_links)\n",
    "    print(\"len of ent_set: {}\".format(len(ent_set)))\n",
    "    # Used to store the best alignment (left is key, right is value) and best probability for such an alignment.\n",
    "    res_dict = {}\n",
    "    p_dict = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        i = 0\n",
    "        for line in file:\n",
    "            line = line.strip('\\n').split('\\t')\n",
    "            if len(line) != 3:\n",
    "                continue\n",
    "            e1 = line[0].replace('dbp:', 'http://dbpedia.org/')\n",
    "            # TODO: This line should rather be: e2 = line[1].replace('y2:', '')\n",
    "            e2 = line[1]\n",
    "            p = float(line[2])\n",
    "            if e1 not in ent_set and e2 not in ent_set:\n",
    "                continue\n",
    "            if e1 not in p_dict or e1 in p_dict and p_dict[e1] < p:\n",
    "                p_dict[e1] = p\n",
    "                res_dict[e1] = e2\n",
    "            i += 1\n",
    "        print(\"Number of alignments outputted by paris: {}\".format(i))\n",
    "        for k, v in res_dict.items():\n",
    "            pair_sim_set.add((k, v))\n",
    "    return pair_sim_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we do exactly the same steps to replicate the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of alignments that they obtain is 70002 when replacing `y2:`, and 67834 when they do not. Note, it is exactly the number of alignments we obtain when we do the or operation, or if we forget the second column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alignments with | among first and second column: 70002\n",
      "Alignments forgetting second column: 67834\n"
     ]
    }
   ],
   "source": [
    "print(\"Alignments with | among first and second column: {}\".format(\n",
    "    len(eqv_i_full_df.loc[eqv_i_full_df['DB1'].isin(test_links_df['DB1']) | eqv_i_full_df['DB2'].isin(test_links_df['DB2'])])))\n",
    "print(\"Alignments forgetting second column: {}\".format(\n",
    "    len(eqv_i_full_df.loc[eqv_i_full_df['DB1'].isin(test_links_df['DB1'])])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We believe the best choice is of course to keep the replace `y2:` with \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we keep only alignments for the best probability in the first column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DB1</th>\n",
       "      <th>DB2</th>\n",
       "      <th>Probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://dbpedia.org/resource/Menahem_Golan</td>\n",
       "      <td>Menahem_Golan</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://dbpedia.org/resource/Snow_White_(1987_f...</td>\n",
       "      <td>Snow_White_(1987_film)</td>\n",
       "      <td>0.509527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://dbpedia.org/resource/Germi_County</td>\n",
       "      <td>Germi_County</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://dbpedia.org/resource/All_Hell's_Breakin...</td>\n",
       "      <td>All_Hell's_Breakin'_Loose</td>\n",
       "      <td>0.677578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>http://dbpedia.org/resource/Lick_It_Up_(song)</td>\n",
       "      <td>Lick_It_Up_(song)</td>\n",
       "      <td>0.857566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96886</th>\n",
       "      <td>http://dbpedia.org/resource/Joyce_Dickerson</td>\n",
       "      <td>Joyce_Dickerson</td>\n",
       "      <td>0.740688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96887</th>\n",
       "      <td>http://dbpedia.org/resource/Divinas_palabras_(...</td>\n",
       "      <td>Airbag_(film)</td>\n",
       "      <td>0.300062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96888</th>\n",
       "      <td>http://dbpedia.org/resource/Jay_Leach_(ice_hoc...</td>\n",
       "      <td>Robertas_Ringys</td>\n",
       "      <td>0.671825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96889</th>\n",
       "      <td>http://dbpedia.org/resource/Five_Ashore_in_Sin...</td>\n",
       "      <td>Five_Ashore_in_Singapore</td>\n",
       "      <td>0.715384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96890</th>\n",
       "      <td>http://dbpedia.org/resource/Wólka_Proszewska</td>\n",
       "      <td>Albinów,_Siedlce_County</td>\n",
       "      <td>0.186680</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70002 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     DB1  \\\n",
       "0              http://dbpedia.org/resource/Menahem_Golan   \n",
       "1      http://dbpedia.org/resource/Snow_White_(1987_f...   \n",
       "2               http://dbpedia.org/resource/Germi_County   \n",
       "4      http://dbpedia.org/resource/All_Hell's_Breakin...   \n",
       "5          http://dbpedia.org/resource/Lick_It_Up_(song)   \n",
       "...                                                  ...   \n",
       "96886        http://dbpedia.org/resource/Joyce_Dickerson   \n",
       "96887  http://dbpedia.org/resource/Divinas_palabras_(...   \n",
       "96888  http://dbpedia.org/resource/Jay_Leach_(ice_hoc...   \n",
       "96889  http://dbpedia.org/resource/Five_Ashore_in_Sin...   \n",
       "96890       http://dbpedia.org/resource/Wólka_Proszewska   \n",
       "\n",
       "                             DB2  Probability  \n",
       "0                  Menahem_Golan     1.000000  \n",
       "1         Snow_White_(1987_film)     0.509527  \n",
       "2                   Germi_County     1.000000  \n",
       "4      All_Hell's_Breakin'_Loose     0.677578  \n",
       "5              Lick_It_Up_(song)     0.857566  \n",
       "...                          ...          ...  \n",
       "96886            Joyce_Dickerson     0.740688  \n",
       "96887              Airbag_(film)     0.300062  \n",
       "96888            Robertas_Ringys     0.671825  \n",
       "96889   Five_Ashore_in_Singapore     0.715384  \n",
       "96890    Albinów,_Siedlce_County     0.186680  \n",
       "\n",
       "[70002 rows x 3 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eqv_i_full_df_in_test = eqv_i_full_df.loc[eqv_i_full_df['DB1'].isin(test_links_df['DB1']) | eqv_i_full_df['DB2'].isin(test_links_df['DB2'])]\n",
    "eqv_i_full_df_in_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_dict = {}\n",
    "res_dict = {}\n",
    "for i, line in eqv_i_full_df_in_test.iterrows():\n",
    "    e1 = line['DB1']\n",
    "    # TODO: This line should rather be: e2 = line[1].replace('y2:', '')\n",
    "    e2 = line['DB2']\n",
    "    p = float(line['Probability'])\n",
    "    if e1 not in p_dict or e1 in p_dict and p_dict[e1] < p:\n",
    "        p_dict[e1] = p\n",
    "        res_dict[e1] = e2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DB1</th>\n",
       "      <th>DB2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://dbpedia.org/resource/Menahem_Golan</td>\n",
       "      <td>Menahem_Golan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://dbpedia.org/resource/Snow_White_(1987_f...</td>\n",
       "      <td>Snow_White_(1987_film)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://dbpedia.org/resource/Germi_County</td>\n",
       "      <td>Germi_County</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://dbpedia.org/resource/All_Hell's_Breakin...</td>\n",
       "      <td>All_Hell's_Breakin'_Loose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://dbpedia.org/resource/Lick_It_Up_(song)</td>\n",
       "      <td>Lick_It_Up_(song)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69997</th>\n",
       "      <td>http://dbpedia.org/resource/Joyce_Dickerson</td>\n",
       "      <td>Joyce_Dickerson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69998</th>\n",
       "      <td>http://dbpedia.org/resource/Divinas_palabras_(...</td>\n",
       "      <td>Airbag_(film)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69999</th>\n",
       "      <td>http://dbpedia.org/resource/Jay_Leach_(ice_hoc...</td>\n",
       "      <td>Robertas_Ringys</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70000</th>\n",
       "      <td>http://dbpedia.org/resource/Five_Ashore_in_Sin...</td>\n",
       "      <td>Five_Ashore_in_Singapore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70001</th>\n",
       "      <td>http://dbpedia.org/resource/Wólka_Proszewska</td>\n",
       "      <td>Albinów,_Siedlce_County</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70002 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     DB1  \\\n",
       "0              http://dbpedia.org/resource/Menahem_Golan   \n",
       "1      http://dbpedia.org/resource/Snow_White_(1987_f...   \n",
       "2               http://dbpedia.org/resource/Germi_County   \n",
       "3      http://dbpedia.org/resource/All_Hell's_Breakin...   \n",
       "4          http://dbpedia.org/resource/Lick_It_Up_(song)   \n",
       "...                                                  ...   \n",
       "69997        http://dbpedia.org/resource/Joyce_Dickerson   \n",
       "69998  http://dbpedia.org/resource/Divinas_palabras_(...   \n",
       "69999  http://dbpedia.org/resource/Jay_Leach_(ice_hoc...   \n",
       "70000  http://dbpedia.org/resource/Five_Ashore_in_Sin...   \n",
       "70001       http://dbpedia.org/resource/Wólka_Proszewska   \n",
       "\n",
       "                             DB2  \n",
       "0                  Menahem_Golan  \n",
       "1         Snow_White_(1987_film)  \n",
       "2                   Germi_County  \n",
       "3      All_Hell's_Breakin'_Loose  \n",
       "4              Lick_It_Up_(song)  \n",
       "...                          ...  \n",
       "69997            Joyce_Dickerson  \n",
       "69998              Airbag_(film)  \n",
       "69999            Robertas_Ringys  \n",
       "70000   Five_Ashore_in_Singapore  \n",
       "70001    Albinów,_Siedlce_County  \n",
       "\n",
       "[70002 rows x 2 columns]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = {\"DB1\": [], \"DB2\": []}\n",
    "for k in res_dict:\n",
    "    res['DB1'].append(k)\n",
    "    res['DB2'].append(res_dict[k])\n",
    "res_df = pd.DataFrame(res)\n",
    "res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we try to understand if there are duplicates that may affect precision: \n",
    "# in the first column there should be no duplicates (it is the key for a set!)\n",
    "np.sum(res_df['DB1'].duplicated())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7776"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The second column instead may have some:\n",
    "np.sum(res_df['DB2'].duplicated())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By this inference it looks like the difference among `eqv_full` preprocessed with Zequn's pipeline and `eqv` is more deeper than only the fact that there are duplicates in the second column. (When running `eqv`, the number of tuples is 60734, which is much lower than 70002 - 7776 = 62226)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
